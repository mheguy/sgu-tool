{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import httpx\n",
    "\n",
    "from sgu_tool.main import (\n",
    "    ensure_directories,\n",
    "    get_podcast_episodes,\n",
    "    get_rss_feed_entries,\n",
    "    load_models,\n",
    "    merge_transcript_and_diarization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting...\")\n",
    "ensure_directories()\n",
    "whisper_model, pipeline = load_models()\n",
    "\n",
    "async with httpx.AsyncClient(follow_redirects=True) as client:\n",
    "    feed_entries = await get_rss_feed_entries(client)\n",
    "    episodes = get_podcast_episodes(feed_entries)\n",
    "\n",
    "    for episode in episodes:\n",
    "        audio_file = await episode.get_audio_file(client)\n",
    "\n",
    "        transcription = episode.get_transcription(audio_file, whisper_model)\n",
    "        episode.transcription_file.write_text(json.dumps(transcription))\n",
    "        print(\"Transcription saved.\")\n",
    "\n",
    "        # TODO: Get some stats from the transcription to feed to diarization (ex. max number of speakers)\n",
    "\n",
    "        diarization = episode.get_diarization(audio_file, pipeline)\n",
    "        episode.diarization_file.write_text(json.dumps(diarization))\n",
    "        print(\"Diarization saved.\")\n",
    "\n",
    "        diarized_transcript = merge_transcript_and_diarization(transcription, diarization)\n",
    "        episode.diarized_transcript_file.write_text(json.dumps(diarized_transcript))\n",
    "        print(\"Diarized transcript saved.\")\n",
    "\n",
    "        # Maybe upload it somewhere or something?\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from sgu_tool.main import Transcription\n",
    "\n",
    "transcription: Transcription = json.loads(Path(\"../data/transcriptions/0889.json\").read_text(\"utf-8\"))\n",
    "intro_text = \" \".join(s[\"text\"] for s in transcription[\"segments\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_transformers\n",
    "import en_core_web_trf\n",
    "import spacy\n",
    "\n",
    "nlp = en_core_web_trf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(intro_text)\n",
    "\n",
    "names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "valid_names: list[str] = []\n",
    "\n",
    "for name in names:\n",
    "    # Name already in the list or less specific than another name\n",
    "    if any(valid_name.startswith(name) for valid_name in valid_names):\n",
    "        continue\n",
    "\n",
    "    # Name is more specific than a one we have (replace it)\n",
    "    if any(name.startswith(valid_name) for valid_name in valid_names):\n",
    "        for index, valid_name in enumerate(valid_names):\n",
    "            if name.startswith(valid_name):\n",
    "                valid_names[index] = name\n",
    "                break\n",
    "        continue\n",
    "\n",
    "    # Name seems to be unique\n",
    "    valid_names.append(name)\n",
    "\n",
    "valid_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
